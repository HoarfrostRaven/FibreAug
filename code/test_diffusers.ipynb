{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u3nrt148Rgif"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3P7iQbKURgil"
      },
      "source": [
        "Login to your Hugging Face account (create one [here](https://hf.co/join) if you don't already have one!). You can login from a notebook and enter your token when prompted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "af1huQq4Rgil"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnF3T2wuRgil"
      },
      "source": [
        "Or login in from the terminal:\n",
        "\n",
        "```bash\n",
        "huggingface-cli login\n",
        "```\n",
        "\n",
        "Since the model checkpoints are quite large, install [Git-LFS](https://git-lfs.com/) to version these large files:\n",
        "\n",
        "```bash\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create repo if not exist, trained model will be pushed to Hugging Face Model Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import create_repo\n",
        "\n",
        "# create_repo(\"HoarfrostRaven/ddpm-sprites-16\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create dataset if not exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"imagefolder\", data_dir=\"C:\\\\Applications\\\\Projets\\\\FibreAug\\\\dataset\\\\raw_data\\\\images\")\n",
        "# dataset.push_to_hub(\"HoarfrostRaven/fibers\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NyxTX8ZMRgim"
      },
      "source": [
        "## Training configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ARVdJxW-Rgim"
      },
      "source": [
        "For convenience, create a `TrainingConfig` class containing the training hyperparameters :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RO7kvwEqRgim"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = [128, 128]  # Dimensions must be a multiple of `2 ** (len(block_out_channels) - 1)`\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = 200\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 5\n",
        "    save_model_epochs = 5\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"ddpm-fibres-128\"  # the model name locally and on the HF Hub\n",
        "\n",
        "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
        "    hub_private_repo = False\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "\n",
        "\n",
        "config = TrainingConfig()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0R51WVeqRgin"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3131HD1HRgin"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset parquet (C:/Users/CZLZ9814/.cache/huggingface/datasets/HoarfrostRaven___parquet/HoarfrostRaven--sprites-a9d8345178ba6140/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "config.dataset_name = \"HoarfrostRaven/fibres_128\"\n",
        "dataset = load_dataset(config.dataset_name, split=\"train\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DVMYLgRsRgio"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\CZLZ9814\\AppData\\Local\\Temp\\1\\ipykernel_19908\\3278871621.py:7: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABOwAAAEhCAYAAADMCz9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAR5UlEQVR4nO3af4zX910H8DflCh1nu2vLjx7coB0cpWaGckcr7Zjp1OpWepHpXKP+oTENLI3ZjIkpicbGJTbTP8xSG/UaMrc/XNLoFraDpaZuNhuzSDko2SyF6/Un3MFBx3G0/Fhbv/7BH4ZoLOT13n1e9+Xx+Jv3k+f3x73v8sxnVqvVahUAAAAAIIWrmi4AAAAAAPwPgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIl0NF3gSvVgz8ZwxuOrHg5n7DixM5xRw4b568MZf/DiX4Yzth7eFs4Arkw17vV24S7lUvV2d4UzPnbb4niRNnFT17ymK6RydPJM0xXS+P6BsXDGyPhkvAi8jzW3LAhnfHLNsgpN2oe7sK59rxyftgxP2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEulousBM9GDPxnDG46seDmfsOLEznJFFjddS4z2NfrZbD28LdwCmX5Z7vYYMvxtqvJ/uUy7V0ckz4YybuuZVaNK8dnovaryWduG9YKbo7e4KZ3zstsXxIvBT1H1957T9X56wAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIBGDHQAAAAAk0tF0gSY82LMxdP6T8z8a7rDjxM5wxpePfDOcwcWin230u1VKKVsPbwtnwJWkxs/d46seDmfUuNfbxZprV4Uz3KfQjKOTZ5quABBS4x67qWtehSZx7uQrmyfsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJdDRd4Er15SPfbLpCKaWU7rnzm65QSill/PyJcEaN9/T3l/xaOAOYXjXuj9/Y/8fhjHa6P46ef7PpCmXNtavCGQ/2bAxnbD28LZxBfkcnz4TO39Q1r1ITsnmvN3b++HOx7xbQjOjvBXKaaZ+rJ+wAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIZFar1Wo1XWK6LVy0MHT+jv9aWalJTPfc+U1XSGX8/ImmK5TnrjoUzpg4NlGhCcwMD/ZsbLpCKSXH/VFKKffNX990hbay7/SL4Yyth7fFi/BT1dvdlSIj6qaueeGMdSuXV2jSPjo3vBvO+MdHDlRoEjMyPpkiA95Plvt4wR3x+3T2SDiChI5Onmm6QimllG/vffWS/p0n7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiXQ0XeByLVy0sPGM18pkuMOPvvS5cMasX9oczmh9ZzCcUUOW1/KRP3wsdH5haf77WUopE8cmwhnwfjYsWN90hbaz7/SLTVdoK99659+brsA0GBmfDGf0dnc1er6UUtatXB7O2Dy4PZwxuPn+cMav//2ScEYN3/jskaYrVPl+1siA6ZDhPi6llPJcPOJ3/vy2cMbbO3LMLbsOjTZdoYqjk2earlBKmd472RN2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEOpoucKUaGz3YdIW24z1lOgzv2hLO6O9bGi8y56F4RtCO4zubrlBKKWXzvf3hjEc+c084Y8mmvw5ntL4zGM64+5GvhjOipk6darrCBceaLsBMMTI+GTp/X9/N4Q7Do2PhDC6W4T2NfrfIb3dnZzijb/bscEbH1FQ4I4MaPzO93V3hjO89NhHO+Iun7whnVPHZeMS23T+MhyRQ4/s1nfe6J+wAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIl0NF3gck0cmwhnLFy0sNHzpZSyeXAonHHkiT8KZ4yNHgxn1FDjtdR4T0uZEzpd4/tZI4Pchve+Hs7o7/vbeJE5D8UzEhg7ebrpCtVkuZOnTp1qukIVNX5f18hwr7e/b+99NZyxdvnicMamX+4LZ3RueDec8Sf3PhfOqGH45fFwxp7RsQpN4P+39733whl9110XzuiYmgpnZLBqSfx3dzsZbpN7bGR8MkXGdPKEHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgkVmtVqvVdImZZqDnrnDGJ+ZeE8546voj4Yx28omTS8IZT50/Fzo/dPjZcAfa3/CuLU1XKKWU0t+3NBYw56E6RRIYWLsynDG4eSCc8St/8y/hDOqaODaRIoPcBu65NZxx828vCmdMDZ8IZ/znv06GM7K46+EV4YxXv3YsdH7omYPhDrS/3Z2dTVcopZTSN3t26HzH1FSlJjG3r14dzvjQ1WfDGYuvvzacUcMvfG5hOON7j8X+lhk7eTrcYWjPoXDGTOMJOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIrNarVar6RLTbaDnrtj5xevCHX7u/JFwxt63jocz2knfzywIZ/xw7pLQ+aGxXeEOQ4efDWfQ/oZ3bWm6QunvWxoPmfNQOOKBpdeEM558/Vw4Y2DtynDGf7wxGc6groljE01XYBoM3HNr8Hz85//swNXhjJeGT4Qz2smK/vnhjA8MvRM6P/TMoXCHoWcOhjNof7s7O5uuUPpmzw5ndExNhTNuX706nPH8/v3hjBp/G2ZxQ39H6PxXB1+o1OTK4gk7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAis1qtVqvpEpdjc+/94YxNyzdUaBLzk/F/a7oC/4c53R9vukJ5YnRHOGNwZHuFJrS74V1bmq5Q/uozXwpnPPn6uXDGhhtuCWd8fdV94Yx9bx8PZ9Sw961Yj6fOxz+TocPPhjO4MgysXVkh49bQ+f5fXBzusHN5jp9/LrZ+dEHo/PB3x8IdhvYcrJBxKJxB+9vd2dl0hbJpxYpwxvP794czfn5gWTjjz4Z+NZzx0vCJcEYN0R6vfu1YuMPQM/G7cKbxhB0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARAx2AAAAAJCIwQ4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIJGOpgs0of+G3tD54R+PhDt85cyZcMbvzZsXzmgnNd7TTcHz0e9WKaWU0XgEXIr+dV8MnX9g6TXhDnfNvzqcMdBzVzije078Pt339vFwRg1rOheEM+Z0fzx0fl24QSmbe+8PZwyObK/QhCtB//LFofPD3x0Ld3jhn8bDGT+7ZWk4o5288MXXwxkfuPGd0Pnod6uUUob2HAxnwKW48+23Q+dvX7063OHmZfF7rEaPuRWu05eGT8RDKljRPz+csX409vfl8KoPhjtsvrc/nDH49HA4Yzp5wg4AAAAAEjHYAQAAAEAiBjsAAAAASMRgBwAAAACJGOwAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEOpoucLkGR7aHMzb33l+hSczZd8+HM/5uKp7BxZ4Y3RELGI13qPEdh0vxwNJrQueffP1c4x343z5//LVwxuDNnwqdH/7xSLgDXKqhPYfCGQNrV1ZoEjN3WW844/uPxv82/OcnPhrOqOHTm35QIWVROGFoz8FGz1/IiH/HaX+3r17ddIXy/P794YwMr6PdfOs394YzNt3ZX6EJl8sTdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARGa1Wq1W0yVmmpXX9YQz1t14W4UmcW/+ZKrpCqWUUm6cc13TFUoppex680Do/KGpw5WaANPJvX6xnnkLKjSJGRzZ3nQFuGQf/vCycEbfmo9UaBJ3cvJU0xVKKaVc3/XBpiuUUkrZu+9HofMvv/xapSbAtHrz0XDEpzf9oEKRuBr3+oqrzlZoEjP49HDTFaadJ+wAAAAAIBGDHQAAAAAkYrADAAAAgEQMdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIl0NF1gJrp1bk844/MLP1WhSdxvHXi06QqllFK+cNvvNl2hlFLKybdON10BuEwDC9aFMw7d+Q8VmsRtOPCn4YyR00fCGV/oaf5OrvG5Dh3fVaEJvL9zZ8+FM05OnqrQJO6VV95ousIFtzRd4IIany0wvRZ3Lwpn3H332gpN4mr8bqhyr9/yoXhGUI3PdWz8WIUm08cTdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARDqaLjATDR3fFc4YWLCuQpO4Q1OHm65QSsnzftT4bIEr0/DpkXDGyOkjFZoA021s/Fg4Y3H3ogpN4mq8lhq8H8BMd3LyVDjjlVfeqNCEmcoTdgAAAACQiMEOAAAAABIx2AEAAABAIgY7AAAAAEjEYAcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIkY7AAAAAAgEYMdAAAAACRisAMAAACARGa1Wq1W0yUAAAAAgAs8YQcAAAAAiRjsAAAAACARgx0AAAAAJGKwAwAAAIBEDHYAAAAAkIjBDgAAAAASMdgBAAAAQCIGOwAAAABIxGAHAAAAAIn8NzcCpPe5Q8IQAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"image\"]):\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7z6gdMRgio"
      },
      "source": [
        "Preprocess the images:\n",
        "\n",
        "* `Resize` changes the image size to the one defined in `config.image_size`.\n",
        "* `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.\n",
        "* `Normalize` is important to rescale the pixel values into a [-1, 1] range, which is what the model expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "huNCaZ02Rgio"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        # transforms.Resize(config.image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_BwfJD8jRgio"
      },
      "source": [
        "Use [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) method to apply the `preprocess` function on images during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "b3RxXcYdRgio"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(Image.fromarray(np.array(image, dtype=np.uint8), 'RGB')) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Revisualize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "C:\\Users\\CZLZ9814\\AppData\\Local\\Temp\\1\\ipykernel_19908\\1434477978.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"images\"]):\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fOI9yHNrRgio"
      },
      "source": [
        "Wrap the dataset in a [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "l4QD7NstRgio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hgqnlA4-Rgio"
      },
      "source": [
        "## Create a UNet2DModel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MCfJEsZTRgip"
      },
      "source": [
        "Create a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.UNet2DModel):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qt5f9oFcRgip"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels, 3 for RGB images\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        # \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        # \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        # \"UpBlock2D\",\n",
        "        # \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# model = UNet2DModel.from_pretrained(\"HoarfrostRaven/ddpm-sprites-16\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Si-QjW3IRgip"
      },
      "source": [
        "Check the sample image shape matches the model output shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lhl7WFENRgip",
        "outputId": "37435ad1-649e-45f8-adb0-8b7079867e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([1, 3, 16, 16])\n",
            "Output shape: torch.Size([1, 3, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(\"Input shape:\", sample_image.shape)\n",
        "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect modules to apply PEFT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# target_modules = []\n",
        "# target_modules = [\n",
        "#     name\n",
        "#     for name, module in model.named_modules()\n",
        "#     if isinstance(module, (nn.Linear, nn.Conv2d, nn.Embedding, nn.ConvTranspose2d))\n",
        "# ]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply PEFT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,537,699 || all params: 30,983,395 || trainable%: 8.190513015116645\n"
          ]
        }
      ],
      "source": [
        "# from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# peft_config = LoraConfig(target_modules=target_modules, # (`Union[List[str],str]`): The names of the modules to apply Lora to.\n",
        "#                          inference_mode=False, # False for train, True for inference\n",
        "#                          r=16, # (`int`): Lora attention dimension.\n",
        "#                          lora_alpha=16, # The alpha parameter for Lora scaling.\n",
        "#                          lora_dropout=0.1, # The dropout probability for Lora layers.\n",
        "#                          bias=\"all\" # (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'\n",
        "# )\n",
        "# model = get_peft_model(model, peft_config)\n",
        "# model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UOgapUbnRgiq"
      },
      "source": [
        "## Create a scheduler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K622XKn0Rgiq"
      },
      "source": [
        "The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a *noise schedule* and an *update rule*.\n",
        "\n",
        "Let's take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the `sample_image` from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bnfTHCj6Rgiq"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAADG0lEQVR4nAEQA+/8APv47/giEDPf6wjm7+bXAT4sVwdZwMRnmrwgefBC4Z0gVgLiAuoTDhoaAdLx+tzv8wHsIOvZ2v0v1R0ZAsJaZ5p8P2q8orYGFhMiGvfS6+xMEUWN16B/t38BNTjxB/8r6dYB7Oj5FA0CDhwUQwZRQvoPGBsQt9XIByvg57v09i0sYBwq4AIYz/e+ksDM9/0DHh7TAhwI6+o0//QN/CYpEB41C4f6wsooAeDDDj4qHezs5q3l0BYDEgwTEyoo0hcM6CLzDgEY6hDxBfL9GO0D+wYoCUcNBBM09fPuLAQct/b5Oi+u9uYqBwq7zMX4ExQREMf86hoA3/jmAufvHgAMPjRRKy0k3ciYD+7yBsOB55yw+gPR6q+sPjwoMz4rHgkGGQAf8fzuAjQWLQYY9PUW6f7RxQkZ/iTesej3Gib7GwEL5v0OKfr7w/f+Khbn0vANMeXv3xHu6QHo8BIM6/8iJNL4ATcdNzEm/MmoaUP4GuHt8kEL6wVRoabl6Ovk9xj7Csn97Psz3hoB/+j47AESGhfi3e8ScCZSQwsYmCDCGQzq7rT7BjMkZvABx/clr8+E5ggdJggF7eP0BCkO3cbTB/UbAmlIaXE1IwnT1u7VNwzwyxYNLfD6uCASTgMLKfRHSxzl2QLrCev13AEi+wn6IwZpET5SO0X40voZBNXt/wzV8QMNIbYB7QIaySz/QU0PFuuD2bWlysX13usC2P3G7Qkb/AsUDgor3wX3pPP0B8byJvDqFOguAeovrivv8RYPJv8rD931DAnhLCERAf3429j/I//45asqcNsaDfHN0wtIAgjDFwH7+O34GxI85wL7HQXb6aH8quSy/O1PAwH11flCBvzUKezx1y0uKvJETEsKxvMNIvavFAEl3AYr9/ee497l7tzgHO0M7CoA4v8AEg8ABxQF8ejrAQb4WhQx2kiS3S6URS8pKu0B6UyQvSqMSCsJ8/3yL8YP9u/b+OXhAAgT8iEC+v4EE/MCDenu7Dn6cUAfKATfCfAA7EYYSkcjJvsNAPP1FMcF9wER8v3r74wLfC6tisa1AAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=16x16>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "# Convert the noisy image tensor to a PIL Image object for visualization\n",
        "# Rearrange the dimensions of the tensor to match the expected image format\n",
        "# Scale the pixel values from the range [-1, 1] to [0, 255] and convert to uint8 type\n",
        "# Extract the first image from the batch and convert it to a numpy array\n",
        "# Create a PIL Image object from the numpy array and display it\n",
        "image_to_show = ((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0]\n",
        "\n",
        "# Create a PIL Image object from the numpy array\n",
        "pil_image = Image.fromarray(image_to_show)\n",
        "\n",
        "# Resize the image to make it larger (e.g., 200x200)\n",
        "# Change the 'new_size' tuple to the desired size\n",
        "new_size = (300, 300)\n",
        "resized_image = pil_image.resize(new_size)\n",
        "\n",
        "# Display the resized image\n",
        "resized_image.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ws5grjRgiq"
      },
      "source": [
        "The training objective of the model is to predict the noise added to the image. The loss at this step can be calculated by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mk9gKoKfRgiq"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "noise_pred = model(noisy_image.to(device), timesteps.to(device)).sample\n",
        "loss = F.mse_loss(noise_pred, noise.to(device))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIbA1sZjRgiq"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-KZP5ZhbRgiq"
      },
      "source": [
        "Construct an optimizer and a learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZM4E0FmmRgiq"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Y10yCwRgiq"
      },
      "source": [
        "Then, you'll need a way to evaluate the model. For evaluation, you can use the [DDPMPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ddpm#diffusers.DDPMPipeline) to generate a batch of sample images and save it as a grid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8_efoSZ4Rgir"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "import math\n",
        "import os\n",
        "\n",
        "\n",
        "def make_grid(images, rows, cols):\n",
        "    w, h = images[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, image in enumerate(images):\n",
        "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.manual_seed(config.seed),\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mDvYcj9TRgir"
      },
      "source": [
        "To upload the model to the Hub, write a function to get your repository name and information and then push it to the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "def get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n",
        "    if token is None:\n",
        "        token = HfFolder.get_token()\n",
        "    if organization is not None:\n",
        "        return f\"{organization}/{model_id}\"\n",
        "    username = whoami(token)[\"name\"]\n",
        "    return f\"{username}/{model_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Ic-eDb4XRgir"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from huggingface_hub import Repository\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.push_to_hub:\n",
        "            repo_name = get_full_repo_name(Path(config.output_dir).name)\n",
        "            repo = Repository(config.output_dir, clone_from=repo_name)\n",
        "        elif config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "    \n",
        "    # Register the LR scheduler\n",
        "    accelerator.register_for_checkpointing(lr_scheduler)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[\"images\"]\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
        "            ).long()\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                if config.push_to_hub:\n",
        "                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n",
        "                else:\n",
        "                    pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LnIf9E67Rgiu"
      },
      "source": [
        "Launch the training with Accelerate's [notebook_launcher](https://huggingface.co/docs/accelerate/main/en/package_reference/launchers#accelerate.notebook_launcher) function. Pass the function the training loop, all the training arguments, and the number of processes (you can change this value to the number of GPUs available to you) to use for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QfjzI5bERgiu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on CPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/HoarfrostRaven/ddpm-sprites-16 into local empty directory.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e55006862b4d49879e2ff8b52dfba49d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5588 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m      3\u001b[0m args \u001b[39m=\u001b[39m (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\u001b[1;32m----> 5\u001b[0m notebook_launcher(train_loop, args, num_processes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\launchers.py:156\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[1;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLaunching training on CPU.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m function(\u001b[39m*\u001b[39;49margs)\n",
            "Cell \u001b[1;32mIn[49], line 57\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\u001b[0m\n\u001b[0;32m     55\u001b[0m noise_pred \u001b[39m=\u001b[39m model(noisy_images, timesteps, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     56\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[1;32m---> 57\u001b[0m accelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m     59\u001b[0m accelerator\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m     60\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:1841\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1839\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1840\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1841\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsdjcCFRgiv"
      },
      "source": [
        "Once training is complete, take a look at the final images generated by diffusion model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk2YCrp-Rgiv"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the image using PIL\n",
        "pil_image = Image.open(sample_images[-1])\n",
        "\n",
        "# Resize the image to make it larger (e.g., 200x200)\n",
        "# Change the 'new_size' tuple to the desired size\n",
        "new_size = (500, 500)\n",
        "resized_image = pil_image.resize(new_size)\n",
        "\n",
        "# Display the resized image\n",
        "resized_image.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
