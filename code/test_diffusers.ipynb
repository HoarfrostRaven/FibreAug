{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u3nrt148Rgif"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3P7iQbKURgil"
      },
      "source": [
        "Login to your Hugging Face account (create one [here](https://hf.co/join) if you don't already have one!). You can login from a notebook and enter your token when prompted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af1huQq4Rgil"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnF3T2wuRgil"
      },
      "source": [
        "Or login in from the terminal:\n",
        "\n",
        "```bash\n",
        "huggingface-cli login\n",
        "```\n",
        "\n",
        "Since the model checkpoints are quite large, install [Git-LFS](https://git-lfs.com/) to version these large files:\n",
        "\n",
        "```bash\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create repo if not exist, trained model will be pushed to Hugging Face Model Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import create_repo\n",
        "\n",
        "# create_repo(\"HoarfrostRaven/ddpm-fibre-16\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create dataset if not exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"imagefolder\", data_dir=\"C:\\\\Applications\\\\Projets\\\\FibreAug\\\\dataset\\\\raw_data\\\\images\")\n",
        "# dataset.push_to_hub(\"HoarfrostRaven/fibers\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NyxTX8ZMRgim"
      },
      "source": [
        "## Training configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ARVdJxW-Rgim"
      },
      "source": [
        "For convenience, create a `TrainingConfig` class containing the training hyperparameters :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RO7kvwEqRgim"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = [16, 16]  # Dimensions must be a multiple of `2 ** (len(block_out_channels) - 1)`\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = 1\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 1\n",
        "    save_model_epochs = 1\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"ddpm-fibre-16\"  # the model name locally and on the HF Hub\n",
        "\n",
        "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
        "    hub_private_repo = False\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "\n",
        "\n",
        "config = TrainingConfig()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0R51WVeqRgin"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3131HD1HRgin"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8030b57fc5b45308326f3b927f454f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/392 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset None/None to C:/Users/CZLZ9814/.cache/huggingface/datasets/HoarfrostRaven___parquet/HoarfrostRaven--sprites-a9d8345178ba6140/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c10b4435ce5c481c9d52c50af2933207",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69a7392be1c74031917868165a024466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/28.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9a18515866947a5878803e1d70dedb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bf904edc2e147a3b09c61ae95e3454e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/89400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset parquet downloaded and prepared to C:/Users/CZLZ9814/.cache/huggingface/datasets/HoarfrostRaven___parquet/HoarfrostRaven--sprites-a9d8345178ba6140/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "config.dataset_name = \"HoarfrostRaven/sprites\"\n",
        "dataset = load_dataset(config.dataset_name, split=\"train\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DVMYLgRsRgio"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\CZLZ9814\\AppData\\Local\\Temp\\1\\ipykernel_10896\\3278871621.py:7: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"image\"]):\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7z6gdMRgio"
      },
      "source": [
        "Preprocess the images:\n",
        "\n",
        "* `Resize` changes the image size to the one defined in `config.image_size`.\n",
        "* `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.\n",
        "* `Normalize` is important to rescale the pixel values into a [-1, 1] range, which is what the model expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huNCaZ02Rgio"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        # transforms.Resize(config.image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_BwfJD8jRgio"
      },
      "source": [
        "Use [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) method to apply the `preprocess` function on images during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3RxXcYdRgio"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Revisualize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"images\"]):\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fOI9yHNrRgio"
      },
      "source": [
        "Wrap the dataset in a [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QD7NstRgio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hgqnlA4-Rgio"
      },
      "source": [
        "## Create a UNet2DModel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MCfJEsZTRgip"
      },
      "source": [
        "Create a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.UNet2DModel):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt5f9oFcRgip"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "# model = UNet2DModel(\n",
        "#     sample_size=config.image_size,  # the target image resolution\n",
        "#     in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "#     out_channels=3,  # the number of output channels, 3 for RGB images\n",
        "#     layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "#     block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "#     down_block_types=(\n",
        "#         \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "#         \"DownBlock2D\",\n",
        "#         \"DownBlock2D\",\n",
        "#         \"DownBlock2D\",\n",
        "#         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "#         \"DownBlock2D\",\n",
        "#     ),\n",
        "#     up_block_types=(\n",
        "#         \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "#         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "#         \"UpBlock2D\",\n",
        "#         \"UpBlock2D\",\n",
        "#         \"UpBlock2D\",\n",
        "#         \"UpBlock2D\",\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "model = UNet2DModel.from_pretrained(\"google/ddpm-ema-celebahq-256\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Si-QjW3IRgip"
      },
      "source": [
        "Check the sample image shape matches the model output shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhl7WFENRgip",
        "outputId": "37435ad1-649e-45f8-adb0-8b7079867e7f"
      },
      "outputs": [],
      "source": [
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(\"Input shape:\", sample_image.shape)\n",
        "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect modules to apply PEFT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "target_modules = []\n",
        "target_modules = [\n",
        "    name\n",
        "    for name, module in model.named_modules()\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d, nn.Embedding, nn.ConvTranspose2d))\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply PEFT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(target_modules=target_modules, # (`Union[List[str],str]`): The names of the modules to apply Lora to.\n",
        "                         inference_mode=False, # False for train, True for inference\n",
        "                         r=16, # (`int`): Lora attention dimension.\n",
        "                         lora_alpha=16, # The alpha parameter for Lora scaling.\n",
        "                         lora_dropout=0.1, # The dropout probability for Lora layers.\n",
        "                         bias=\"all\" # (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.to(device)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UOgapUbnRgiq"
      },
      "source": [
        "## Create a scheduler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K622XKn0Rgiq"
      },
      "source": [
        "The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a *noise schedule* and an *update rule*.\n",
        "\n",
        "Let's take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the `sample_image` from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnfTHCj6Rgiq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "# Convert the noisy image tensor to a PIL Image object for visualization\n",
        "# Rearrange the dimensions of the tensor to match the expected image format\n",
        "# Scale the pixel values from the range [-1, 1] to [0, 255] and convert to uint8 type\n",
        "# Extract the first image from the batch and convert it to a numpy array\n",
        "# Create a PIL Image object from the numpy array and display it\n",
        "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ws5grjRgiq"
      },
      "source": [
        "The training objective of the model is to predict the noise added to the image. The loss at this step can be calculated by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk9gKoKfRgiq"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "noise_pred = model(noisy_image, timesteps).sample\n",
        "loss = F.mse_loss(noise_pred, noise)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIbA1sZjRgiq"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-KZP5ZhbRgiq"
      },
      "source": [
        "Construct an optimizer and a learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM4E0FmmRgiq"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Y10yCwRgiq"
      },
      "source": [
        "Then, you'll need a way to evaluate the model. For evaluation, you can use the [DDPMPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ddpm#diffusers.DDPMPipeline) to generate a batch of sample images and save it as a grid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_efoSZ4Rgir"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "import math\n",
        "import os\n",
        "\n",
        "\n",
        "def make_grid(images, rows, cols):\n",
        "    w, h = images[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, image in enumerate(images):\n",
        "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.manual_seed(config.seed),\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mDvYcj9TRgir"
      },
      "source": [
        "To upload the model to the Hub, write a function to get your repository name and information and then push it to the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "def get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n",
        "    if token is None:\n",
        "        token = HfFolder.get_token()\n",
        "    if organization is not None:\n",
        "        return f\"{organization}/{model_id}\"\n",
        "    username = whoami(token)[\"name\"]\n",
        "    return f\"{username}/{model_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic-eDb4XRgir"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from huggingface_hub import Repository\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.push_to_hub:\n",
        "            repo_name = get_full_repo_name(Path(config.output_dir).name)\n",
        "            repo = Repository(config.output_dir, clone_from=repo_name)\n",
        "        elif config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[\"images\"]\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
        "            ).long()\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                if config.push_to_hub:\n",
        "                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n",
        "                else:\n",
        "                    pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LnIf9E67Rgiu"
      },
      "source": [
        "Launch the training with Accelerate's [notebook_launcher](https://huggingface.co/docs/accelerate/main/en/package_reference/launchers#accelerate.notebook_launcher) function. Pass the function the training loop, all the training arguments, and the number of processes (you can change this value to the number of GPUs available to you) to use for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfjzI5bERgiu"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsdjcCFRgiv"
      },
      "source": [
        "Once training is complete, take a look at the final images generated by diffusion model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk2YCrp-Rgiv"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
        "Image.open(sample_images[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
